[["index.html", "R and Stats For Beginners by Beginners Chapter 1 Introduction 1.1 Who is this book for? 1.2 What is the aim? 1.3 micro-white paper", " R and Stats For Beginners by Beginners Excavating Mind 2022-12-09 Chapter 1 Introduction 1.1 Who is this book for? This book is designed for psychology students who have an interest in improving their mathematical understanding. 1.2 What is the aim? This book, while in continuous development, will constantly add new content to improve students mathematical understanding of statistics. 1.3 micro-white paper If you would like further detail please go to the micro-white paper: https://excavating-mind.netlify.app/post/r-statswhitepaper/rstatswpp/ "],["summary-statistics.html", "Chapter 2 Summary statistics 2.1 Mathmatics 2.2 In R", " Chapter 2 Summary statistics 2.1 Mathmatics 2.1.1 Mean The mean is generally presented in statistics as \\(\\overline{X}\\). The formula for the mean is: \\(\\overline{X} = \\frac{\\sum_i^n{x_i}}{n}\\) Essentially, all data points are added together and then divided by the total number of data points. For example, say you had a dataset of 2, 3, 4, and 7 you would add these together which would equal 16. Then you would divide 16 by the number of data points (4). So, the mean of this dataset would be 4 2.1.2 Median *to be completed 2.1.3 Variance *to be completed 2.1.4 Standard deviation *to be completed 2.2 In R You can work out the mean of a dataset in R in a variety of ways. mean_dataset &lt;- c(8,9,11,13,14) # lets say we have a dataset mean_dataset # lets view the data points ## [1] 8 9 11 13 14 8 + 9 + 11 + 13 + 14 # we now know it is 55 ## [1] 55 55 / 5 # and we now know the mean is 11 ## [1] 11 The above example is fine if you have a few numbers, but what if your dataset is larger? Well, you can use certain packages to make this easier library(tidyverse) # lets load our library head(mpg) # we are using the mpg dataset that is built into the tidyverse package ## # A tibble: 6 x 11 ## manufac~1 model displ year cyl trans drv cty hwy fl ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto~ f 18 29 p ## 2 audi a4 1.8 1999 4 manu~ f 21 29 p ## 3 audi a4 2 2008 4 manu~ f 20 31 p ## 4 audi a4 2 2008 4 auto~ f 21 30 p ## 5 audi a4 2.8 1999 6 auto~ f 16 26 p ## 6 audi a4 2.8 1999 6 manu~ f 18 26 p ## # ... with 1 more variable: class &lt;chr&gt;, and abbreviated ## # variable name 1: manufacturer lets say we want to work out the mean year that cars are manufactured mpg %&gt;% # pipe function summarise(mean_year = mean(year)) # summarise function in dplyr package ## # A tibble: 1 x 1 ## mean_year ## &lt;dbl&gt; ## 1 2004. Now we know that the approximate mean year that cars are manufactured is around 2003-04. "],["correlation.html", "Chapter 3 Correlation", " Chapter 3 Correlation to do "],["linear-regression.html", "Chapter 4 Linear regression 4.1 Mathematical understanding of simple linear regression 4.2 In R", " Chapter 4 Linear regression 4.1 Mathematical understanding of simple linear regression 4.1.1 brief introduction Simple linear regression, or linear regression, is a statistical method that can examine the influence of one independent variable on one dependent variable. The formula for linear regression is as follows: \\(Y = b_{0} + b_{1}X + error\\) While this can look confusing, each component of this equation can be reduced to a simpler explanation. \\(Y\\) = dependent variable (also known as the outcome variable) \\(X\\) = independent variable (also known as the predictor variable) \\(b_{0}\\) = regression constant (also known as the slope) \\(b_{1}\\) = regression coefficient (also known as the intercept) \\(error\\) = measure of residuals which is the difference between the predicted and actual values. Some of these variables are familiar, e.g., a dependent variable. However, you may wonder what a slope and intercept are. 4.1.1.0.1 slope \\(b_{1} = \\frac{cov_{(x, y)}}{s^2_{x}}\\) Now, let us remember how to calculate covariance \\(cov(x,y) = \\frac{\\sum_i^n(x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\\) here, each participant calculation is added together and divided by the number of participants minus one. In addition, \\(s^2\\) refers to the variance of the independent variable. And here is the formula \\(s^2 = \\frac{\\sum_i^n(x_{i} - \\overline{x})^2}{n-1}\\) here, the sum of each participants x variable values (score minus the mean) is summed and divided by the number of participants minus one. 4.1.1.1 intercept \\(b_{0} = \\overline{y} - b_{1} \\times \\overline{x}\\) Simply put, the intercept is the mean of the dependent variable minus the slope multiplied by the mean of the independent variable. Thus, to calculate a simple linear regression, the above must be known. 4.2 In R While the theoretical understanding of the simple linear regression is important, it is also advantageous to know how to conduct this in R. first, we need to import our libraries needed for this analysis. library(tidyverse) # main collection of tidy packages including modelr library(performance) # to test model assumptions library(NHANES) # dataset library(Hmisc) # correlation packages head(NHANES) # examining the NHANES dataset ## # A tibble: 6 x 76 ## ID SurveyYr Gender Age AgeDe~1 AgeMo~2 Race1 Race3 Educa~3 ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 51624 2009_10 male 34 &quot; 30-3~ 409 White &lt;NA&gt; High S~ ## 2 51624 2009_10 male 34 &quot; 30-3~ 409 White &lt;NA&gt; High S~ ## 3 51624 2009_10 male 34 &quot; 30-3~ 409 White &lt;NA&gt; High S~ ## 4 51625 2009_10 male 4 &quot; 0-9&quot; 49 Other &lt;NA&gt; &lt;NA&gt; ## 5 51630 2009_10 female 49 &quot; 40-4~ 596 White &lt;NA&gt; Some C~ ## 6 51638 2009_10 male 9 &quot; 0-9&quot; 115 White &lt;NA&gt; &lt;NA&gt; ## # ... with 67 more variables: MaritalStatus &lt;fct&gt;, ## # HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;, Poverty &lt;dbl&gt;, ## # HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;, ## # Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;, ## # BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, ## # BPSysAve &lt;int&gt;, BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, ## # BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;, BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, ... here, we have a lot of continous/integer data; and, we can therefore implement SLR. Now, let us see if a regression analysis is worthwhile, one way to do so would be to examine the correlation between variables. Specifically, if a variable is correlated to another variable, we could then predict the value of a variable based on another. Here we will do a simple BMI vs weight. rcorr(NHANES$BMI, NHANES$Weight) ## x y ## x 1.0 0.9 ## y 0.9 1.0 ## ## n ## x y ## x 9634 9634 ## y 9634 9922 ## ## P ## x y ## x 0 ## y 0 We can see that BMI and weight are highly correlated, r = .09. We want to see if weight can predict BMI. So lets build some models! Typically it is common to build a first model that simply uses the mean of the outcome variable as the predictor and a second model testing our predictor. We can then compare these models and examine whether our predictor model is significantly better than prediction based on mean. model1 &lt;- lm(BMI ~ 1, data = NHANES) model2 &lt;- lm(BMI ~ Weight, data = NHANES) lets test our model assumptions. check_model(model2) While not perfect, the assumptions approximately hold up. Now, let us see if our model2 is better than our model1 anova(model1, model2) ## Analysis of Variance Table ## ## Model 1: BMI ~ 1 ## Model 2: BMI ~ Weight ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9633 524169 ## 2 9632 97528 1 426641 42136 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here, we can see that our model2 is significantly better p &lt; .001 now let us summarise our model summary(model2) ## ## Call: ## lm(formula = BMI ~ Weight, data = NHANES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.1964 -2.2379 -0.3448 1.8328 23.5453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.093256 0.091514 99.36 &lt;2e-16 *** ## Weight 0.241376 0.001176 205.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.182 on 9632 degrees of freedom ## (366 observations deleted due to missingness) ## Multiple R-squared: 0.8139, Adjusted R-squared: 0.8139 ## F-statistic: 4.214e+04 on 1 and 9632 DF, p-value: &lt; 2.2e-16 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

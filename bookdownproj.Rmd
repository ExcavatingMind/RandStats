--- 
title: "R and Stats"
subtitle: "A beginners guide by a beginner"
author: "Excavating Mind"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Introduction 

## Who is this book for?
This book is designed for psychology students who have an interest in improving their mathematical understanding.

## What is the aim?
This book, while in continuous development, will constantly add new content to improve students' mathematical understanding of statistics. 

## micro-white paper

If you would like further detail please go to the micro-white paper: https://excavating-mind.netlify.app/post/r-statswhitepaper/rstatswpp/




<!--chapter:end:index.Rmd-->

# Summary statistics

## Mathmatics

### Mean

The mean is generally presented in statistics as $\overline{X}$. The formula for the mean is:

$\overline{X} = \frac{\sum_i^n{x_i}}{n}$

Essentially, all data points are added together and then divided by the total number of data points.

For example, say you had a dataset of 2, 3, 4, and 7

you would add these together which would equal 16.

Then you would divide 16 by the number of data points (4).
So, the mean of this dataset would be 4


### Median

*to be completed


### Variance

*to be completed


### Standard deviation

*to be completed


# In R

You can work out the mean of a dataset in R in a variety of ways.

```{r}
mean_dataset <- c(8,9,11,13,14) # lets say we have a dataset

mean_dataset # lets view the data points

8 + 9 + 11 + 13 + 14 # we now know it is 55

55 / 5 # and we now know the mean is 11


```
The above example is fine if you have a few numbers, but what if your dataset is larger?
Well, you can use certain packages to make this easier

```{r , warning=FALSE, message=FALSE}
library(tidyverse) # lets load our library
```

```{r}
head(mpg) # we are using the mpg dataset that is built into the tidyverse package
```
lets say we want to work out the mean year that cars are manufactured

```{r}
mpg %>% # pipe function
  summarise(mean_year = mean(year)) # summarise function in dplyr package
```
Now we know that the approximate mean year that cars are manufactured is around 2003-04.



<!--chapter:end:02-tears.Rmd-->

# Correlation

* to do

<!--chapter:end:03-race.Rmd-->

# Linear regression

## Mathematical understanding of simple linear regression

### brief introduction

Simple linear regression, or linear regression, is a statistical method that can examine the influence of **one** independent variable on **one** dependent variable. The formula for linear regression is as follows:

$Y = b_{0} + b_{1}X + error$

While this can look confusing, each component of this equation can be reduced to a simpler explanation. 

- $Y$ = dependent variable (also known as the outcome variable)
- $X$ = independent variable (also known as the predictor variable)
- $b_{0}$ = regression constant (also known as the slope)
- $b_{1}$ = regression coefficient (also known as the intercept)
- $error$ = measure of residuals which is the difference between the predicted and actual values.

Some of these variables are familiar, e.g., a dependent variable. However, you may wonder what a slope and intercept are. 

##### slope

$b_{1} = \frac{cov_{(x, y)}}{s^2_{x}}$

Now, let us remember how to calculate covariance

$cov(x,y) = \frac{\sum_i^n(x_i - \overline{x})(y_i - \overline{y})}{n-1}$

here, each participant calculation is added together and divided by the number of participants minus one.


In addition, $s^2$ refers to the variance of the independent variable. And here is the formula

$s^2 = \frac{\sum_i^n(x_{i} - \overline{x})^2}{n-1}$

here, the sum of each participants x variable values (score minus the mean) is summed and divided by the number of participants minus one. 


#### intercept 

$b_{0} = \overline{y} - b_{1} \times \overline{x}$

Simply put, the intercept is the mean of the dependent variable minus the slope multiplied by the mean of the independent variable.

Thus, to calculate a simple linear regression, the above must be known.


## In R

While the theoretical understanding of the simple linear regression is important, it is also advantageous to know how to conduct this in R. 


first, we need to import our libraries needed for this analysis.

```{r , warning=FALSE, error=FALSE, message=FALSE}
library(tidyverse) # main collection of tidy packages including modelr
library(performance) # to test model assumptions
library(NHANES) # dataset
library(Hmisc) # correlation packages
```



```{r}
head(NHANES) # examining the NHANES dataset
```
here, we have a lot of continous/integer data; and, we can therefore implement SLR. 


Now, let us see if a regression analysis is worthwhile, one way to do so would be to examine the correlation between variables. Specifically, if a variable is correlated to another variable, we could then predict the value of a variable based on another.

Here we will do a simple BMI vs weight.

```{r}
rcorr(NHANES$BMI, NHANES$Weight)
```


We can see that BMI and weight are highly correlated, ***r*** = .09. We want to see if weight can predict BMI. So lets build some models!
Typically it is common to build a first model that simply uses the mean of the outcome variable as the predictor and a second model testing our predictor. We can then compare these models and examine whether our predictor model is significantly better than prediction based on mean.

```{r}
model1 <- lm(BMI ~ 1, data = NHANES)
model2 <- lm(BMI ~ Weight, data = NHANES)
```

lets test our model assumptions. 
```{r}
check_model(model2)
```



While not perfect, the assumptions approximately hold up.

Now, let us see if our model2 is better than our model1

```{r}
anova(model1, model2)
```
Here, we can see that our model2 is significantly better ***p*** < .001

now let us summarise our model

```{r}
summary(model2)
```


<!--chapter:end:04-linear-regression.Rmd-->


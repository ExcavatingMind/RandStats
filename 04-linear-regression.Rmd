# Linear regression

## Mathematical understanding of simple linear regression

### brief introduction

Simple linear regression, or linear regression, is a statistical method that can examine the influence of **one** independent variable on **one** dependent variable. The formula for linear regression is as follows:

$Y = b_{0} + b_{1}X + error$

While this can look confusing, each component of this equation can be reduced to a simpler explanation. 

- $Y$ = dependent variable (also known as the outcome variable)
- $X$ = independent variable (also known as the predictor variable)
- $b_{0}$ = regression constant (also known as the slope)
- $b_{1}$ = regression coefficient (also known as the intercept)
- $error$ = measure of residuals which is the difference between the predicted and actual values.

Some of these variables are familiar, e.g., a dependent variable. However, you may wonder what a slope and intercept are. 

##### slope

$b_{1} = \frac{cov_{(x, y)}}{s^2_{x}}$

Now, let us remember how to calculate covariance

$cov(x,y) = \frac{\sum_i^n(x_i - \overline{x})(y_i - \overline{y})}{n-1}$

here, each participant calculation is added together and divided by the number of participants minus one.


In addition, $s^2$ refers to the variance of the independent variable. And here is the formula

$s^2 = \frac{\sum_i^n(x_{i} - \overline{x})^2}{n-1}$

here, the sum of each participants x variable values (score minus the mean) is summed and divided by the number of participants minus one. 


#### intercept 

$b_{0} = \overline{y} - b_{1} \times \overline{x}$

Simply put, the intercept is the mean of the dependent variable minus the slope multiplied by the mean of the independent variable.

Thus, to calculate a simple linear regression, the above must be known.


## In R

While the theoretical understanding of the simple linear regression is important, it is also advantageous to know how to conduct this in R. 


first, we need to import our libraries needed for this analysis.

```{r , warning=FALSE, error=FALSE, message=FALSE}
library(tidyverse) # main collection of tidy packages including modelr
library(performance) # to test model assumptions
library(NHANES) # dataset
library(Hmisc) # correlation packages
```



```{r}
head(NHANES) # examining the NHANES dataset
```
here, we have a lot of continous/integer data; and, we can therefore implement SLR. 


Now, let us see if a regression analysis is worthwhile, one way to do so would be to examine the correlation between variables. Specifically, if a variable is correlated to another variable, we could then predict the value of a variable based on another.

Here we will do a simple BMI vs weight.

```{r}
rcorr(NHANES$BMI, NHANES$Weight)
```


We can see that BMI and weight are highly correlated, ***r*** = .09. We want to see if weight can predict BMI. So lets build some models!
Typically it is common to build a first model that simply uses the mean of the outcome variable as the predictor and a second model testing our predictor. We can then compare these models and examine whether our predictor model is significantly better than prediction based on mean.

```{r}
model1 <- lm(BMI ~ 1, data = NHANES)
model2 <- lm(BMI ~ Weight, data = NHANES)
```

lets test our model assumptions. 
```{r}
check_model(model2)
```



While not perfect, the assumptions approximately hold up.

Now, let us see if our model2 is better than our model1

```{r}
anova(model1, model2)
```
Here, we can see that our model2 is significantly better ***p*** < .001

now let us summarise our model

```{r}
summary(model2)
```

